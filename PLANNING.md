# ğŸ§ Top Songs Dashboard - Project Planning

## ğŸ“‹ Project Overview

The Top Songs Dashboard is a learning-focused, end-to-end data engineering and analytics platform that simulates and processes real-time user song play data. The dashboard displays the **Top 10 songs** for each **region**, updated **hourly** and **daily**.

## ğŸ—ï¸ Architecture

The system is composed of microservices and batch/streaming components:

```css
[Simulated Clients]
         â”‚
         â–¼
[FastAPI Web Server]
         â”‚
         â–¼
      [Kafka]
         â”‚
         â–¼
[Apache Spark Structured Streaming]
         â”‚
         â–¼
  [MinIO (S3-compatible)]
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                         â–¼
[ETL Jobs via Prefect]     [Aggregation Jobs via Prefect]
         â”‚                         â”‚
         â–¼                         â–¼
[Enriched Data in S3]     [Top N Songs in PostgreSQL]
                                   â”‚
                                   â–¼
                        [ReactJS Dashboard App]

```

## ğŸ”§ Tech Stack

| Layer               | Technology                             |
|--------------------|-----------------------------------------|
| Data Simulation     | Python + OpenAI API                     |
| API Server          | FastAPI                                 |
| Messaging Queue     | Apache Kafka (confluent-kafka)          |
| Streaming Processor | Apache Spark Structured Streaming       |
| Object Storage      | MinIO (S3-compatible) via boto3         |
| Workflow Orchestration | Prefect                             |
| Serving DB          | PostgreSQL with asyncpg                 |
| CLI Interface       | Typer + Rich                            |
| Dashboard Frontend  | ReactJS + Chart.js                      |
| Containerization    | Docker + Docker Compose                 |
| Python Package Mgmt | `uv` (for virtualenv + dependency mgmt) |

## ğŸ”„ Data Flow

1. **Simulated clients** send song play events with metadata via REST API
2. **FastAPI server** pushes events to **Kafka**
3. **Spark** reads events from Kafka, processes them, and writes to **MinIO** in partitioned Parquet files
4. **Prefect** runs:
   - Enrichment jobs to add metadata (joins transactional and master data)
   - Aggregation jobs to calculate top songs
   - Loading jobs to update PostgreSQL
5. **ReactJS Dashboard** queries PostgreSQL and renders top 10 songs per region/hour/day

## ğŸ“ Project Structure

```
top_songs_dashboard/
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ .python-version             # Python version lock (for pyenv or uv)
â”œâ”€â”€ pyproject.toml              # Dependency and build configuration using uv
â”œâ”€â”€ README.md                   # Project overview and usage
â”œâ”€â”€ PLANNING.md                 # Project architecture, goals, and structure
â”œâ”€â”€ TASK.md                     # Task tracking and progress
â”œâ”€â”€ docker/                     # Dockerfiles and container build setup
â”œâ”€â”€ docs/                       # Project documentation, plans, and diagrams
â”œâ”€â”€ tests/                      # Unit and integration tests

â”œâ”€â”€ top_songs/                  # Main application Python package
â”‚
â”‚   â”œâ”€â”€ cli/                    # CLI commands (e.g., Typer-based interfaces)
â”‚
â”‚   â”œâ”€â”€ core/                   # Core shared logic across domains
â”‚   â”‚   â”œâ”€â”€ config/             # App settings, environment loader, global config
â”‚   â”‚   â”œâ”€â”€ connectivity/       # Service connectivity checks
â”‚   â”‚   â”œâ”€â”€ models/             # Shared domain models and schemas
â”‚   â”‚   â””â”€â”€ utils/              # Reusable utilities (logging, validation, etc.)
â”‚
â”‚   â”œâ”€â”€ ingestion/              # Ingestion layer for incoming data
â”‚   â”‚   â”œâ”€â”€ api/                # FastAPI service for receiving event data
â”‚   â”‚   â””â”€â”€ simulator/          # Simulated client that generates song play events
â”‚
â”‚   â”œâ”€â”€ orchestration/          # Workflow orchestration logic
â”‚   â”‚   â””â”€â”€ flows/              # Prefect flows for data pipelines
â”‚
â”‚   â”œâ”€â”€ processing/             # Data processing jobs
â”‚   â”‚   â”œâ”€â”€ enrichment/         # Metadata enhancement, lookups, etc.
â”‚   â”‚   â””â”€â”€ streaming/          # Real-time processing jobs (e.g., Spark, Kafka)
â”‚
â”‚   â”œâ”€â”€ serving/                # Data serving layer
â”‚   â”‚   â”œâ”€â”€ api/                # Optional API layer for exposing processed data
â”‚   â”‚   â””â”€â”€ dashboard/          # React frontend for visualizing top songs
â”‚
â”‚   â”œâ”€â”€ storage/                # Data persistence and storage layer
â”‚   â”‚   â”œâ”€â”€ database/           # PostgreSQL-related logic (schemas, access)
â”‚   â”‚   â””â”€â”€ object_store/       # S3/MinIO integration for data lake storage
â”‚   
â”‚   â””â”€â”€ streaming/              # Data streaming / transport layer
â”‚       â””â”€â”€ kafka.py            # Kafka interface for sending/receiving messages
```

## ğŸ§ª Development Setup

- All services containerized with **Docker**
- Single `docker-compose.yaml` file for local orchestration
- MinIO replaces AWS S3 with no code changes
- Python dependencies managed via **uv**
- CLI interface for checking service connectivity

## ğŸ“š Coding Style & Conventions

- **Language**: Python is the primary backend language
- **Style**: Follow PEP8
- **Type Hints**: Use Python type hints for all functions
- **Formatting**: Code should be formatted with `black`
- **Data Validation**: Use `pydantic` for data validation
- **APIs**: Use `FastAPI` for APIs
- **Database**: Use `asyncpg` for asynchronous PostgreSQL access
- **Documentation**: Write Google-style docstrings for every function
- **Code Organization**: Maintain modularity and single responsibility principle
- **File Size**: No file should exceed 500 lines of code
- **Interfaces**: Implement consistent interfaces across storage and streaming services

## ğŸ§  Project Goals

- Gain hands-on experience with distributed systems (Kafka, Spark)
- Explore real-time streaming architectures
- Practice data orchestration and analytics pipelines
- Design a robust and observable system
- Build a professional-grade data product with modularity and maintainability

## ğŸš€ Future Enhancements

- Add Redis caching for low-latency dashboard
- Add Prometheus + Grafana for observability
- Add user authentication (e.g., Auth0 or Firebase)
- Deploy on k8s (minikube or kind) for CI/CD training
- Implement comprehensive test suite for all interfaces 